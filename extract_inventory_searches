#!/usr/bin/env python
# -*- coding: utf-8 -*-

import argparse
import pandas as pd
import sys

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textacy import preprocess
from urllib.parse import urlparse

parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument('inventory', type=argparse.FileType('r'))
parser.add_argument('searches', type=argparse.FileType('r'))

args = parser.parse_args()
wnl = WordNetLemmatizer()

inventory_df = pd.read_csv(args.inventory, encoding='utf-8', usecols=['URL'])
inventory_df['Path'] = inventory_df['URL'].apply(lambda url: urlparse(url).path)

searches_df = pd.read_csv(
    args.searches,
    encoding='utf-8',
    skiprows=6,
    usecols=['Search Destination Page', 'Search Term'],
)

merged_df = pd.merge(
    how='left',
    left=searches_df,
    left_on='Search Destination Page',
    right=inventory_df,
    right_on='Path',
)

merged_df.dropna(axis=0, how='any', inplace=True)

merged_df['Term'] = searches_df['Search Term'] \
    .map(lambda t: str(t)) \
    .map(lambda t: preprocess.preprocess_text(t,
                                              fix_unicode=True,
                                              lowercase=True,
                                              no_accents=True,
                                              no_contractions=True,
                                              no_currency_symbols=True,
                                              no_emails=True,
                                              no_numbers=True,
                                              no_phone_numbers=True,
                                              no_punct=True,
                                              no_urls=True,
                                              transliterate=True)) \
    .map(lambda t: t.split()) \
    .map(lambda t: filter(lambda t: t not in stopwords.words('english'), t)) \
    .map(lambda t: map(wnl.lemmatize, t)) \
    .map(lambda t: ' '.join(t))

merged_df \
    .drop(['Search Destination Page', 'Search Term', 'URL'], axis=1) \
    .drop_duplicates(subset=['Term']) \
    .sort_values('Term', ascending=True) \
    .to_csv(sys.stdout, encoding='utf-8', index=False)
